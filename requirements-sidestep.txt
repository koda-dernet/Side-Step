# ╔══════════════════════════════════════════════════════════════╗
# ║  Side-Step -- Extra dependencies for ACE-Step LoRA training ║
# ║  Install: pip install -r requirements-sidestep.txt          ║
# ╚══════════════════════════════════════════════════════════════╝
#
# Side-Step is a companion CLI/TUI for ACE-Step.
# Install ACE-Step's own requirements first, then these.

# ── Required for Side-Step CLI ────────────────────────────────
rich>=13.0.0

# ── Required for Side-Step TUI ────────────────────────────────
textual>=0.47.0

# ── Optional: 8-bit optimizers (saves ~30-40% optimizer VRAM) ─
# Uncomment to enable AdamW8bit in the optimizer selector.
# Supports Linux and Windows (official wheels).
# bitsandbytes>=0.45.0

# ── Optional: Prodigy adaptive optimizer (auto-tunes LR) ─────
# Uncomment to enable the Prodigy optimizer.
# Great if you don't want to manually tune learning rate.
# prodigyopt>=1.1.2

# ── Optional: LoKR adapter support (LyCORIS) ────────────────
# Uncomment to enable LoKR (Kronecker) adapters alongside LoRA.
# Uses LyCORIS library for adapter injection.
# lycoris-lora>=2.0.0

# ── Optional: Flash Attention 2 (faster training on Ampere+) ─
# Requires CUDA GPU with compute capability >= 8.0 (RTX 3090+).
# Linux: builds from source (needs CUDA toolkit, ~10-20 min).
# Windows: use pre-built wheel from ACE-Step-1.5 requirements.
# flash-attn
